{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "4842e21e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import sys\n",
    "\n",
    "os.environ[\"SPARK_HOME\"] = \"/home/talentum/spark\"\n",
    "os.environ[\"PYLIB\"] = os.environ[\"SPARK_HOME\"] + \"/python/lib\"\n",
    "# In below two lines, use /usr/bin/python2.7 if you want to use Python 2\n",
    "os.environ[\"PYSPARK_PYTHON\"] = \"/usr/bin/python3.6\" \n",
    "os.environ[\"PYSPARK_DRIVER_PYTHON\"] = \"/usr/bin/python3\"\n",
    "sys.path.insert(0, os.environ[\"PYLIB\"] +\"/py4j-0.10.7-src.zip\")\n",
    "sys.path.insert(0, os.environ[\"PYLIB\"] +\"/pyspark.zip\")\n",
    "\n",
    "# NOTE: Whichever package you want mention here.\n",
    "# os.environ['PYSPARK_SUBMIT_ARGS'] = '--packages com.databricks:spark-xml_2.11:0.6.0 pyspark-shell' \n",
    "# os.environ['PYSPARK_SUBMIT_ARGS'] = '--packages org.apache.spark:spark-avro_2.11:2.4.0 pyspark-shell'\n",
    "os.environ['PYSPARK_SUBMIT_ARGS'] = '--packages com.databricks:spark-xml_2.11:0.6.0,org.apache.spark:spark-avro_2.11:2.4.3 pyspark-shell'\n",
    "# os.environ['PYSPARK_SUBMIT_ARGS'] = '--packages com.databricks:spark-xml_2.11:0.6.0,org.apache.spark:spark-avro_2.11:2.4.0 pyspark-shell'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b52dd270",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "7239a58b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import SparkSession class\n",
    "# SparkSession is the entry point to use Spark SQL and DataFrame API\n",
    "from pyspark.sql import SparkSession\n",
    "\n",
    "# Import commonly used Spark SQL functions\n",
    "# col   -> used to refer to a column in a DataFrame\n",
    "# when  -> used for conditional logic (similar to if-else)\n",
    "# count -> used for counting rows or values\n",
    "# size  -> used to get the size of an array-type column\n",
    "from pyspark.sql.functions import col, when, count, size\n",
    "\n",
    "\n",
    "# Create a SparkSession object\n",
    "# This starts a Spark application and connects to the Spark cluster\n",
    "spark = SparkSession.builder\\\n",
    "    .appName(\"AmazonAppliancesIngestionNotebook\").getOrCreate()                                   \n",
    "# Name of the Spark application (useful for monitoring)\n",
    "# Creates a new session or returns existing one\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "844454fa",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- asin: string (nullable = true)\n",
      " |-- image: array (nullable = true)\n",
      " |    |-- element: string (containsNull = true)\n",
      " |-- overall: double (nullable = true)\n",
      " |-- reviewText: string (nullable = true)\n",
      " |-- reviewTime: string (nullable = true)\n",
      " |-- reviewerID: string (nullable = true)\n",
      " |-- reviewerName: string (nullable = true)\n",
      " |-- style: struct (nullable = true)\n",
      " |    |-- Color:: string (nullable = true)\n",
      " |    |-- Design:: string (nullable = true)\n",
      " |    |-- Flavor:: string (nullable = true)\n",
      " |    |-- Format:: string (nullable = true)\n",
      " |    |-- Item Package Quantity:: string (nullable = true)\n",
      " |    |-- Length:: string (nullable = true)\n",
      " |    |-- Package Quantity:: string (nullable = true)\n",
      " |    |-- Package Type:: string (nullable = true)\n",
      " |    |-- Pattern:: string (nullable = true)\n",
      " |    |-- Scent:: string (nullable = true)\n",
      " |    |-- Size Name:: string (nullable = true)\n",
      " |    |-- Size:: string (nullable = true)\n",
      " |    |-- Style Name:: string (nullable = true)\n",
      " |    |-- Style:: string (nullable = true)\n",
      " |-- summary: string (nullable = true)\n",
      " |-- unixReviewTime: long (nullable = true)\n",
      " |-- verified: boolean (nullable = true)\n",
      " |-- vote: string (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Read the JSON file containing raw Amazon appliance reviews\n",
    "# spark.read.json() loads the JSON data into a Spark DataFrame\n",
    "# file:/// is used to read data from the local file system\n",
    "reviews_df = spark.read.json(\n",
    "    \"file:///home/talentum/projects/AmazonReviewAnalytics/BigDataPipeline/data/raw/Appliances.json\"\n",
    ")\n",
    "\n",
    "# Print the schema (structure) of the DataFrame\n",
    "# This shows column names, data types, and whether fields are nullable\n",
    "# Useful for understanding data before cleaning and processing\n",
    "reviews_df.printSchema()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "0f8cec78",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------+--------+----------+-------+--------------------+--------------------+--------------+\n",
      "|    reviewerID|verified|      asin|overall|          reviewText|             summary|unixReviewTime|\n",
      "+--------------+--------+----------+-------+--------------------+--------------------+--------------+\n",
      "|A3NHUQ33CFH3VM|   false|1118461304|    5.0|Not one thing in ...|Clear on what lea...|    1385510400|\n",
      "|A3SK6VNBQDNBJE|   false|1118461304|    5.0|I have enjoyed Dr...|Becoming more inn...|    1383264000|\n",
      "|A3SOFHUR27FO3K|   false|1118461304|    5.0|Alan Gregerman be...|The World from Di...|    1381363200|\n",
      "|A1HOG1PYCAE157|   false|1118461304|    5.0|Alan Gregerman is...|Strangers are You...|    1381276800|\n",
      "|A26JGAM6GZMM4V|   false|1118461304|    5.0|As I began to rea...|How and why it is...|    1378512000|\n",
      "+--------------+--------+----------+-------+--------------------+--------------------+--------------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Select only the required columns from the original reviews DataFrame\n",
    "# This helps in removing unnecessary data and reducing memory usage\n",
    "reviews_df_clean = reviews_df.select(\n",
    "    \"reviewerID\",        # Unique ID of the reviewer\n",
    "    \"verified\",          # Indicates whether the purchase was verified (true/false)\n",
    "    \"asin\",              # Amazon Standard Identification Number (product ID)\n",
    "    \"overall\",           # Rating given by the user (1 to 5)\n",
    "    \"reviewText\",        # Full text of the review\n",
    "    \"summary\",           # Short summary/title of the review\n",
    "    \"unixReviewTime\"     # Review time in Unix timestamp format\n",
    ")\n",
    "\n",
    "# Display the first 5 rows of the cleaned DataFrame\n",
    "# Useful for quick validation of data after column selection\n",
    "reviews_df_clean.show(5)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "f0308178",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+--------+----+-------+----------+-------+--------------+\n",
      "|reviewerID|verified|asin|overall|reviewText|summary|unixReviewTime|\n",
      "+----------+--------+----+-------+----------+-------+--------------+\n",
      "|         0|       0|   0|      0|         0|      0|             0|\n",
      "+----------+--------+----+-------+----------+-------+--------------+\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "602777"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Function to generate a condition for detecting \"empty\" values\n",
    "# It behaves differently based on the column data type\n",
    "def get_empty_condition(column_name, data_type):\n",
    "\n",
    "    # If the column is of String type\n",
    "    # Empty value means an empty string (\"\")\n",
    "    if data_type == 'string':\n",
    "        return col(column_name) == \"\"\n",
    "\n",
    "    # If the column is an Array type\n",
    "    # Empty value means array size is 0 (no elements)\n",
    "    elif 'array' in data_type:\n",
    "        return size(col(column_name)) == 0\n",
    "\n",
    "    # For all other data types (int, float, boolean, etc.)\n",
    "    # These types cannot be empty, only NULL\n",
    "    else:\n",
    "        return col(column_name).isNull()\n",
    "\n",
    "\n",
    "# Apply empty-value check dynamically on all columns\n",
    "# reviews_df_clean.dtypes gives a list of (column_name, data_type)\n",
    "# when() applies condition\n",
    "# count() counts how many rows satisfy the condition\n",
    "reviews_df_clean.select([\n",
    "    count(\n",
    "        when(get_empty_condition(c, t), c)\n",
    "    ).alias(c)          # Column name is kept same for easy understanding\n",
    "    for c, t in reviews_df_clean.dtypes\n",
    "]).show()\n",
    "\n",
    "\n",
    "# Count total number of rows in the DataFrame\n",
    "# Useful for validation and comparison with empty/null counts\n",
    "reviews_df_clean.count()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "ca2d87b3",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "563870"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Filter the reviews DataFrame to keep only verified purchases\n",
    "# verified == \"true\" ensures we consider only genuine customer reviews\n",
    "reviews_final = reviews_df_clean.filter(\n",
    "    col(\"verified\") == \"true\"\n",
    ")\n",
    "\n",
    "# Count the number of rows after filtering\n",
    "# This helps in validating how many verified reviews are available\n",
    "reviews_final.count()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "73d0452d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Write the cleaned reviews DataFrame to disk in Parquet format\n",
    "# Parquet is a columnar storage format and is efficient for big data processing\n",
    "reviews_final.write.mode(\"overwrite\").parquet(\n",
    "        \"file:///home/talentum/projects/AmazonReviewAnalytics/BigDataPipeline/data/cleaned/Reviews_Appliances_Parquet\"\n",
    "    )\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dc137d59",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
