{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "yyNx8HDnaKt0"
      },
      "outputs": [],
      "source": [
        "# Install the spaCy library\n",
        "# spaCy is an industry-grade NLP library used for fast and efficient text processing\n",
        "!pip install spacy\n",
        "\n",
        "# Download the small English language model for spaCy\n",
        "# This model provides tokenization, lemmatization, stopwords, and basic NLP features\n",
        "!python -m spacy download en_core_web_sm\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "lvhMkUJqo68o"
      },
      "outputs": [],
      "source": [
        "# Import pandas library for data manipulation and analysis\n",
        "# Pandas helps us work with datasets in table format (rows and columns)\n",
        "import pandas as pd\n",
        "\n",
        "\n",
        "# Import the spaCy library\n",
        "# spaCy provides an efficient NLP pipeline for text processing\n",
        "import spacy\n",
        "\n",
        "# Load the small English language model\n",
        "# This model includes tokenizer, lemmatizer, stopwords, and basic linguistic rules\n",
        "nlp = spacy.load(\"en_core_web_sm\")\n",
        "\n",
        "# Import Google Drive utility from Google Colab\n",
        "# This allows Colab to access files stored in our Google Drive\n",
        "from google.colab import drive\n",
        "\n",
        "# Mount Google Drive to the Colab environment\n",
        "# After mounting, Drive files will be accessible under /content/drive\n",
        "drive.mount('/content/drive')\n",
        "\n",
        "# Load spaCy English model with unnecessary components disabled\n",
        "# Disabling parser, tagger, and NER greatly improves performance\n",
        "nlp = spacy.load(\n",
        "    \"en_core_web_sm\",\n",
        "    disable=[\"parser\", \"ner\", \"tagger\"]\n",
        ")\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "LU8uPsZxphrX"
      },
      "outputs": [],
      "source": [
        "# Path where the curated Parquet folder is stored in Google Drive\n",
        "# This folder was downloaded from HDFS (Gold layer) and uploaded to Drive\n",
        "folder_path = '/content/drive/MyDrive/appliance_reviews_curated/'\n",
        "\n",
        "# Read the Parquet dataset into a Pandas DataFrame\n",
        "# Pandas can read a folder containing Parquet part files\n",
        "# This is safe because Parquet is columnar and schema-aware\n",
        "df = pd.read_parquet(folder_path)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "u6V8mpemqAac"
      },
      "source": [
        "Create Sentiment Labels (RULE-BASED)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "gjvQlYPsqBK-"
      },
      "outputs": [],
      "source": [
        "# Define a function to convert numeric ratings into sentiment labels\n",
        "# We use this because the dataset does not already have sentiment (positive/negative) tags\n",
        "def label_sentiment(rating):\n",
        "\n",
        "    # Check if the rating value is greater than or equal to 4\n",
        "    # Ratings 4 and 5 generally mean the customer is satisfied\n",
        "    if rating >= 4:\n",
        "        # Return 'positive' sentiment for high ratings\n",
        "        return \"positive\"\n",
        "\n",
        "    # Check if the rating value is exactly 3\n",
        "    # Rating 3 is considered neither good nor bad\n",
        "    elif rating == 3:\n",
        "        # Return 'neutral' sentiment for average rating\n",
        "        return \"neutral\"\n",
        "\n",
        "    # If the rating is less than 3 (i.e., 1 or 2)\n",
        "    # These ratings indicate dissatisfaction\n",
        "    else:\n",
        "        # Return 'negative' sentiment for low ratings\n",
        "        return \"negative\"\n",
        "\n",
        "# Apply the label_sentiment function to the 'overall' column of the DataFrame\n",
        "# The 'apply' function runs label_sentiment on each rating value\n",
        "df[\"sentiment\"] = df[\"overall\"].apply(label_sentiment)\n",
        "\n",
        "# Count the number of records in each sentiment category\n",
        "# This helps us understand class distribution (positive, neutral, negative)\n",
        "df[\"sentiment\"].value_counts()\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8A6Q1SfIqHMg"
      },
      "source": [
        "Combine Text Fields"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "CAxvzEWjqH5Q"
      },
      "outputs": [],
      "source": [
        "# Combine the 'summary' and 'reviewText' columns into one single text column\n",
        "# This gives the model more context by using both short and detailed reviews\n",
        "# fillna(\"\") replaces missing values with empty strings to avoid errors during concatenation\n",
        "df[\"text\"] = df[\"summary\"].fillna(\"\") + \" \" + df[\"reviewText\"].fillna(\"\")\n",
        "\n",
        "# Select only the columns required for machine learning\n",
        "# 'text' will be used as the input feature for the model\n",
        "# 'sentiment' will be used as the target label\n",
        "df = df[[\"text\", \"sentiment\"]]\n",
        "\n",
        "# Display the first few rows of the DataFrame\n",
        "# This helps verify that text combination and column selection worked correctly\n",
        "df.head()\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "C-H5kF9ipFd0"
      },
      "source": [
        "Define spaCy-based Text Cleaning Function"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "RZpS0lv-pHS7"
      },
      "outputs": [],
      "source": [
        "# Define a faster text cleaning function using spaCy\n",
        "# Lemmatization is intentionally skipped for performance and DL compatibility\n",
        "def clean_text_spacy_fast(text):\n",
        "    \"\"\"\n",
        "    Fast spaCy-based preprocessing:\n",
        "    - Tokenization\n",
        "    - Lowercasing\n",
        "    - Stopword removal\n",
        "    - Punctuation removal\n",
        "    \"\"\"\n",
        "\n",
        "    # Process text using minimal spaCy pipeline\n",
        "    doc = nlp(text)\n",
        "\n",
        "    # Extract clean tokens (no lemmatization for speed)\n",
        "    tokens = [\n",
        "        token.text.lower()      # Use original token text and lowercase it\n",
        "        for token in doc\n",
        "        if not token.is_stop    # Remove stopwords\n",
        "        and not token.is_punct  # Remove punctuation\n",
        "        and token.is_alpha      # Keep only alphabetic tokens\n",
        "    ]\n",
        "\n",
        "    # Join tokens into a single cleaned string\n",
        "    return \" \".join(tokens)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "o0KuZtrppMhe"
      },
      "source": [
        "Apply spaCy Preprocessing to Dataset"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true,
          "base_uri": "https://localhost:8080/"
        },
        "id": "Sf_iIapG40T7",
        "outputId": "de9c3949-66f9-4c2b-8009-bf8df81ac6d3"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "spaCy preprocessing:   0%|          | 0/564410 [00:00<?, ?it/s]/usr/local/lib/python3.12/dist-packages/spacy/pipeline/lemmatizer.py:188: UserWarning: [W108] The rule-based lemmatizer did not find POS annotation for one or more tokens. Check that your pipeline includes components that assign token.pos, typically 'tagger'+'attribute_ruler' or 'morphologizer'.\n",
            "  warnings.warn(Warnings.W108)\n",
            "spaCy preprocessing: 100%|██████████| 564410/564410 [24:52<00:00, 378.12it/s]\n"
          ]
        }
      ],
      "source": [
        "# Import tqdm to visualize progress\n",
        "from tqdm import tqdm\n",
        "\n",
        "# Create list to store cleaned text\n",
        "cleaned_texts = []\n",
        "\n",
        "# Process text in batches using spaCy's pipe for maximum speed\n",
        "for doc in tqdm(\n",
        "    nlp.pipe(\n",
        "        df[\"text\"],         # Raw review text\n",
        "        batch_size=2000,    # Larger batch size for better throughput\n",
        "        n_process=1         # Single process for Colab stability\n",
        "    ),\n",
        "    total=len(df),\n",
        "    desc=\"spaCy preprocessing\"\n",
        "):\n",
        "    # Extract cleaned tokens\n",
        "    tokens = [\n",
        "        token.text.lower()\n",
        "        for token in doc\n",
        "        if not token.is_stop\n",
        "        and not token.is_punct\n",
        "        and token.is_alpha\n",
        "    ]\n",
        "\n",
        "    # Append cleaned sentence\n",
        "    cleaned_texts.append(\" \".join(tokens))\n",
        "\n",
        "# Assign cleaned text back to DataFrame\n",
        "df[\"clean_text_spacy\"] = cleaned_texts\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true
        },
        "id": "a8JS1aBbECX9",
        "outputId": "56cd51e3-ec62-4518-efad-32cf94c0c425"
      },
      "outputs": [
        {
          "data": {
            "application/vnd.google.colaboratory.intrinsic+json": {
              "summary": "{\n  \"name\": \"df[[\\\"text\\\", \\\"clean_text_spacy\\\"]]\",\n  \"rows\": 5,\n  \"fields\": [\n    {\n      \"column\": \"text\",\n      \"properties\": {\n        \"dtype\": \"string\",\n        \"num_unique_values\": 3,\n        \"samples\": [\n          \"Matched pigtail cord and works great. Just what I needed for my electric range. Matched pigtail cord and works great.\",\n          \"complete package I like the fact that the wire ends have mounting tips assembled and that the cord has a strain relief clamp included.\\nI'd recommend this to any diy person\",\n          \"Perfect Fit Needed another couple of feet with new dryer, perfect fit.  Gives you more options then the shorter cords normally sold.\"\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"clean_text_spacy\",\n      \"properties\": {\n        \"dtype\": \"string\",\n        \"num_unique_values\": 3,\n        \"samples\": [\n          \"matched pigtail cord works great needed electric range matched pigtail cord works great\",\n          \"complete package like fact wire ends mounting tips assembled cord strain relief clamp included recommend diy person\",\n          \"perfect fit needed couple feet new dryer perfect fit gives options shorter cords normally sold\"\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    }\n  ]\n}",
              "type": "dataframe"
            },
            "text/html": [
              "\n",
              "  <div id=\"df-dd46dcd0-d478-40e0-beae-8151d40a23ec\" class=\"colab-df-container\">\n",
              "    <div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>text</th>\n",
              "      <th>clean_text_spacy</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>Matched pigtail cord and works great. Just wha...</td>\n",
              "      <td>matched pigtail cord works great needed electr...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>Matched pigtail cord and works great. Just wha...</td>\n",
              "      <td>matched pigtail cord works great needed electr...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>complete package I like the fact that the wire...</td>\n",
              "      <td>complete package like fact wire ends mounting ...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>complete package I like the fact that the wire...</td>\n",
              "      <td>complete package like fact wire ends mounting ...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>Perfect Fit Needed another couple of feet with...</td>\n",
              "      <td>perfect fit needed couple feet new dryer perfe...</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>\n",
              "    <div class=\"colab-df-buttons\">\n",
              "\n",
              "  <div class=\"colab-df-container\">\n",
              "    <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-dd46dcd0-d478-40e0-beae-8151d40a23ec')\"\n",
              "            title=\"Convert this dataframe to an interactive table.\"\n",
              "            style=\"display:none;\">\n",
              "\n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\" viewBox=\"0 -960 960 960\">\n",
              "    <path d=\"M120-120v-720h720v720H120Zm60-500h600v-160H180v160Zm220 220h160v-160H400v160Zm0 220h160v-160H400v160ZM180-400h160v-160H180v160Zm440 0h160v-160H620v160ZM180-180h160v-160H180v160Zm440 0h160v-160H620v160Z\"/>\n",
              "  </svg>\n",
              "    </button>\n",
              "\n",
              "  <style>\n",
              "    .colab-df-container {\n",
              "      display:flex;\n",
              "      gap: 12px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert {\n",
              "      background-color: #E8F0FE;\n",
              "      border: none;\n",
              "      border-radius: 50%;\n",
              "      cursor: pointer;\n",
              "      display: none;\n",
              "      fill: #1967D2;\n",
              "      height: 32px;\n",
              "      padding: 0 0 0 0;\n",
              "      width: 32px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert:hover {\n",
              "      background-color: #E2EBFA;\n",
              "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "      fill: #174EA6;\n",
              "    }\n",
              "\n",
              "    .colab-df-buttons div {\n",
              "      margin-bottom: 4px;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert {\n",
              "      background-color: #3B4455;\n",
              "      fill: #D2E3FC;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert:hover {\n",
              "      background-color: #434B5C;\n",
              "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "      fill: #FFFFFF;\n",
              "    }\n",
              "  </style>\n",
              "\n",
              "    <script>\n",
              "      const buttonEl =\n",
              "        document.querySelector('#df-dd46dcd0-d478-40e0-beae-8151d40a23ec button.colab-df-convert');\n",
              "      buttonEl.style.display =\n",
              "        google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "      async function convertToInteractive(key) {\n",
              "        const element = document.querySelector('#df-dd46dcd0-d478-40e0-beae-8151d40a23ec');\n",
              "        const dataTable =\n",
              "          await google.colab.kernel.invokeFunction('convertToInteractive',\n",
              "                                                    [key], {});\n",
              "        if (!dataTable) return;\n",
              "\n",
              "        const docLinkHtml = 'Like what you see? Visit the ' +\n",
              "          '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
              "          + ' to learn more about interactive tables.';\n",
              "        element.innerHTML = '';\n",
              "        dataTable['output_type'] = 'display_data';\n",
              "        await google.colab.output.renderOutput(dataTable, element);\n",
              "        const docLink = document.createElement('div');\n",
              "        docLink.innerHTML = docLinkHtml;\n",
              "        element.appendChild(docLink);\n",
              "      }\n",
              "    </script>\n",
              "  </div>\n",
              "\n",
              "\n",
              "    </div>\n",
              "  </div>\n"
            ],
            "text/plain": [
              "                                                text  \\\n",
              "0  Matched pigtail cord and works great. Just wha...   \n",
              "1  Matched pigtail cord and works great. Just wha...   \n",
              "2  complete package I like the fact that the wire...   \n",
              "3  complete package I like the fact that the wire...   \n",
              "4  Perfect Fit Needed another couple of feet with...   \n",
              "\n",
              "                                    clean_text_spacy  \n",
              "0  matched pigtail cord works great needed electr...  \n",
              "1  matched pigtail cord works great needed electr...  \n",
              "2  complete package like fact wire ends mounting ...  \n",
              "3  complete package like fact wire ends mounting ...  \n",
              "4  perfect fit needed couple feet new dryer perfe...  "
            ]
          },
          "execution_count": 10,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "# Display original text and spaCy-cleaned text side by side\n",
        "# This helps visually verify whether preprocessing worked correctly\n",
        "df[[\"text\", \"clean_text_spacy\"]].head(5)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "yRphCp3bFEy9"
      },
      "source": [
        "Encode Sentiment Label"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true
        },
        "id": "7PMKz0JBFFy3",
        "outputId": "66092d0f-4f47-4e9b-e807-1e743a782085"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "{'negative': np.int64(0), 'neutral': np.int64(1), 'positive': np.int64(2)}"
            ]
          },
          "execution_count": 11,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "# Import LabelEncoder from scikit-learn\n",
        "# LabelEncoder converts categorical text labels into numeric form\n",
        "from sklearn.preprocessing import LabelEncoder\n",
        "\n",
        "# Initialize the label encoder\n",
        "# This object will learn a mapping between sentiment labels and numbers\n",
        "label_encoder = LabelEncoder()\n",
        "\n",
        "# Encode the sentiment column into numeric labels\n",
        "# Example mapping (may vary):\n",
        "# negative -> 0, neutral -> 1, positive -> 2\n",
        "df[\"sentiment_encoded\"] = label_encoder.fit_transform(df[\"sentiment\"])\n",
        "\n",
        "# Display the mapping between original labels and encoded values\n",
        "# This is important for interpreting model predictions later\n",
        "dict(zip(label_encoder.classes_, label_encoder.transform(label_encoder.classes_)))\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "58co73aNFNQw"
      },
      "source": [
        "Train–Test Split"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true
        },
        "id": "pE71J6doFN4b",
        "outputId": "ba0bfab3-0bd5-4992-d770-668a59669271"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Training samples: 451528\n",
            "Testing samples: 112882\n"
          ]
        }
      ],
      "source": [
        "# Import train_test_split to divide data into training and testing sets\n",
        "# This helps evaluate how well the LSTM generalizes to unseen data\n",
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "# Define input feature X\n",
        "# We use spaCy-cleaned text as input to the LSTM model\n",
        "X = df[\"clean_text_spacy\"]\n",
        "\n",
        "# Define target variable y\n",
        "# These are the numeric sentiment labels created earlier\n",
        "y = df[\"sentiment_encoded\"]\n",
        "\n",
        "# Split the dataset into training and testing sets\n",
        "# 80% data for training, 20% for testing\n",
        "# stratify=y ensures class balance is maintained in both sets\n",
        "X_train, X_test, y_train, y_test = train_test_split(\n",
        "    X,                 # Input text data\n",
        "    y,                 # Encoded sentiment labels\n",
        "    test_size=0.2,     # Use 20% of data for testing\n",
        "    random_state=42,   # Fixed seed for reproducibility\n",
        "    stratify=y         # Preserve class distribution\n",
        ")\n",
        "\n",
        "# Print the size of train and test sets for confirmation\n",
        "print(\"Training samples:\", len(X_train))\n",
        "print(\"Testing samples:\", len(X_test))\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_-vdX--XFaxp"
      },
      "source": [
        "Tokenize Text Using Keras Tokenizer"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true
        },
        "id": "8nxKD6zHFZ7R",
        "outputId": "bfcd0361-5bdc-4d6e-9020-13da780f3136"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Vocabulary size: 51078\n"
          ]
        }
      ],
      "source": [
        "# Import Tokenizer from Keras\n",
        "# Tokenizer converts text into sequences of integers\n",
        "from tensorflow.keras.preprocessing.text import Tokenizer\n",
        "\n",
        "# Define the maximum number of words to keep in the vocabulary\n",
        "# Keeping top frequent words controls model size and memory usage\n",
        "MAX_VOCAB_SIZE = 20000\n",
        "\n",
        "# Initialize the tokenizer\n",
        "# oov_token handles words not seen during training (Out-Of-Vocabulary)\n",
        "tokenizer = Tokenizer(\n",
        "    num_words=MAX_VOCAB_SIZE,\n",
        "    oov_token=\"<OOV>\"\n",
        ")\n",
        "\n",
        "# Fit the tokenizer ONLY on training text\n",
        "# This learns the word-to-index mapping from training data\n",
        "# Avoids data leakage from test set\n",
        "tokenizer.fit_on_texts(X_train)\n",
        "\n",
        "# Convert training text into sequences of integers\n",
        "X_train_seq = tokenizer.texts_to_sequences(X_train)\n",
        "\n",
        "# Convert test text into sequences using the same tokenizer\n",
        "X_test_seq = tokenizer.texts_to_sequences(X_test)\n",
        "\n",
        "# Print vocabulary size to verify tokenizer learning\n",
        "print(\"Vocabulary size:\", len(tokenizer.word_index))\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_Iiqw6UgFoeg"
      },
      "source": [
        "Pad Sequences (Fixed-Length Input)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true
        },
        "id": "oVtPt1FfFqpO",
        "outputId": "aac8de00-0175-4f62-b0a6-3282e2840093"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Padded training shape: (451528, 100)\n",
            "Padded testing shape: (112882, 100)\n"
          ]
        }
      ],
      "source": [
        "# Import pad_sequences from Keras\n",
        "# pad_sequences is required to make all input sequences the same length\n",
        "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
        "\n",
        "# Re-pad training sequences with the current MAX_SEQUENCE_LENGTH\n",
        "# This ensures consistent input size for the LSTM\n",
        "MAX_SEQUENCE_LENGTH = 100\n",
        "\n",
        "\n",
        "X_train_pad = pad_sequences(\n",
        "    X_train_seq,               # Integer sequences from tokenizer\n",
        "    maxlen=MAX_SEQUENCE_LENGTH,\n",
        "    padding=\"post\",            # Add zeros at the end of shorter sequences\n",
        "    truncating=\"post\"          # Cut extra words from longer sequences\n",
        ")\n",
        "\n",
        "# Re-pad test sequences using the same configuration\n",
        "X_test_pad = pad_sequences(\n",
        "    X_test_seq,\n",
        "    maxlen=MAX_SEQUENCE_LENGTH,\n",
        "    padding=\"post\",\n",
        "    truncating=\"post\"\n",
        ")\n",
        "\n",
        "# Confirm padding shapes\n",
        "print(\"Padded training shape:\", X_train_pad.shape)\n",
        "print(\"Padded testing shape:\", X_test_pad.shape)\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "upviSMavF4z2"
      },
      "source": [
        "Build LSTM Model Architecture"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true
        },
        "id": "Gn0aqUsCF5cs",
        "outputId": "c244bc16-1a9d-4989-8b67-5e842e104847"
      },
      "outputs": [
        {
          "data": {
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\">Model: \"sequential\"</span>\n",
              "</pre>\n"
            ],
            "text/plain": [
              "\u001b[1mModel: \"sequential\"\u001b[0m\n"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">┏━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━┓\n",
              "┃<span style=\"font-weight: bold\"> Layer (type)                    </span>┃<span style=\"font-weight: bold\"> Output Shape           </span>┃<span style=\"font-weight: bold\">       Param # </span>┃\n",
              "┡━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━┩\n",
              "│ embedding (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Embedding</span>)           │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">100</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">64</span>)        │     <span style=\"color: #00af00; text-decoration-color: #00af00\">1,280,000</span> │\n",
              "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
              "│ lstm (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">LSTM</span>)                     │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">64</span>)             │        <span style=\"color: #00af00; text-decoration-color: #00af00\">33,024</span> │\n",
              "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
              "│ dropout (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dropout</span>)               │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">64</span>)             │             <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │\n",
              "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
              "│ dense (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)                   │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">3</span>)              │           <span style=\"color: #00af00; text-decoration-color: #00af00\">195</span> │\n",
              "└─────────────────────────────────┴────────────────────────┴───────────────┘\n",
              "</pre>\n"
            ],
            "text/plain": [
              "┏━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━┓\n",
              "┃\u001b[1m \u001b[0m\u001b[1mLayer (type)                   \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1mOutput Shape          \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1m      Param #\u001b[0m\u001b[1m \u001b[0m┃\n",
              "┡━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━┩\n",
              "│ embedding (\u001b[38;5;33mEmbedding\u001b[0m)           │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m100\u001b[0m, \u001b[38;5;34m64\u001b[0m)        │     \u001b[38;5;34m1,280,000\u001b[0m │\n",
              "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
              "│ lstm (\u001b[38;5;33mLSTM\u001b[0m)                     │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m64\u001b[0m)             │        \u001b[38;5;34m33,024\u001b[0m │\n",
              "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
              "│ dropout (\u001b[38;5;33mDropout\u001b[0m)               │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m64\u001b[0m)             │             \u001b[38;5;34m0\u001b[0m │\n",
              "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
              "│ dense (\u001b[38;5;33mDense\u001b[0m)                   │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m3\u001b[0m)              │           \u001b[38;5;34m195\u001b[0m │\n",
              "└─────────────────────────────────┴────────────────────────┴───────────────┘\n"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Total params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">1,313,219</span> (5.01 MB)\n",
              "</pre>\n"
            ],
            "text/plain": [
              "\u001b[1m Total params: \u001b[0m\u001b[38;5;34m1,313,219\u001b[0m (5.01 MB)\n"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">1,313,219</span> (5.01 MB)\n",
              "</pre>\n"
            ],
            "text/plain": [
              "\u001b[1m Trainable params: \u001b[0m\u001b[38;5;34m1,313,219\u001b[0m (5.01 MB)\n"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Non-trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> (0.00 B)\n",
              "</pre>\n"
            ],
            "text/plain": [
              "\u001b[1m Non-trainable params: \u001b[0m\u001b[38;5;34m0\u001b[0m (0.00 B)\n"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        }
      ],
      "source": [
        "# Import required Keras components\n",
        "from tensorflow.keras.models import Sequential\n",
        "from tensorflow.keras.layers import Embedding, LSTM, Dense, Dropout\n",
        "\n",
        "# Number of sentiment classes\n",
        "NUM_CLASSES = len(label_encoder.classes_)\n",
        "\n",
        "# Define a lighter LSTM model for faster training\n",
        "model = Sequential()\n",
        "\n",
        "# Smaller embedding dimension for speed\n",
        "model.add(\n",
        "    Embedding(\n",
        "        input_dim=MAX_VOCAB_SIZE,\n",
        "        output_dim=64   # Reduced from 128\n",
        "    )\n",
        ")\n",
        "\n",
        "# LSTM with fewer units\n",
        "model.add(\n",
        "    LSTM(\n",
        "        units=64,       # Reduced from 128\n",
        "        return_sequences=False\n",
        "    )\n",
        ")\n",
        "\n",
        "# Dropout for regularization\n",
        "model.add(Dropout(0.5))\n",
        "\n",
        "# Output layer\n",
        "model.add(\n",
        "    Dense(\n",
        "        NUM_CLASSES,\n",
        "        activation=\"softmax\"\n",
        "    )\n",
        ")\n",
        "\n",
        "# Build model explicitly\n",
        "model.build(input_shape=(None, MAX_SEQUENCE_LENGTH))\n",
        "\n",
        "# Show updated summary\n",
        "model.summary()\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "aMAQrh8cGX7O"
      },
      "source": [
        "Compile the LSTM Model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true
        },
        "id": "0DHbRKCwMBZd"
      },
      "outputs": [],
      "source": [
        "# Compile the optimized LSTM model\n",
        "# This step is mandatory before calling model.fit()\n",
        "\n",
        "model.compile(\n",
        "    optimizer=\"adam\",                # Adam optimizer for efficient learning\n",
        "    loss=\"sparse_categorical_crossentropy\",\n",
        "    # Used because labels are integer-encoded and task is multi-class\n",
        "    metrics=[\"accuracy\"]              # Track accuracy during training\n",
        ")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PMChwWG2S6eF"
      },
      "source": [
        "Train the LSTM Model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true
        },
        "id": "Q6idGgJAGZeQ",
        "outputId": "d0d2c437-a1d3-4ba2-d93b-bcf11498cbfc"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 1/15\n",
            "\u001b[1m1764/1764\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m467s\u001b[0m 263ms/step - accuracy: 0.8332 - loss: 0.5653 - val_accuracy: 0.8368 - val_loss: 0.5448\n",
            "Epoch 2/15\n",
            "\u001b[1m1764/1764\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m462s\u001b[0m 262ms/step - accuracy: 0.8383 - loss: 0.5449 - val_accuracy: 0.8373 - val_loss: 0.5430\n",
            "Epoch 3/15\n",
            "\u001b[1m1764/1764\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m461s\u001b[0m 261ms/step - accuracy: 0.8440 - loss: 0.5032 - val_accuracy: 0.8429 - val_loss: 0.4590\n",
            "Epoch 4/15\n",
            "\u001b[1m1764/1764\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m467s\u001b[0m 265ms/step - accuracy: 0.8401 - loss: 0.4431 - val_accuracy: 0.8369 - val_loss: 0.4402\n",
            "Epoch 5/15\n",
            "\u001b[1m1764/1764\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m471s\u001b[0m 267ms/step - accuracy: 0.8422 - loss: 0.4715 - val_accuracy: 0.8925 - val_loss: 0.3932\n",
            "Epoch 6/15\n",
            "\u001b[1m1764/1764\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m456s\u001b[0m 258ms/step - accuracy: 0.8966 - loss: 0.3626 - val_accuracy: 0.9106 - val_loss: 0.2904\n",
            "Epoch 7/15\n",
            "\u001b[1m1764/1764\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m472s\u001b[0m 268ms/step - accuracy: 0.9121 - loss: 0.2798 - val_accuracy: 0.9133 - val_loss: 0.2578\n",
            "Epoch 8/15\n",
            "\u001b[1m1764/1764\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m465s\u001b[0m 264ms/step - accuracy: 0.9179 - loss: 0.2430 - val_accuracy: 0.9141 - val_loss: 0.2524\n",
            "Epoch 9/15\n",
            "\u001b[1m1764/1764\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m462s\u001b[0m 262ms/step - accuracy: 0.9219 - loss: 0.2249 - val_accuracy: 0.9156 - val_loss: 0.2471\n",
            "Epoch 10/15\n",
            "\u001b[1m1764/1764\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m487s\u001b[0m 276ms/step - accuracy: 0.9273 - loss: 0.2063 - val_accuracy: 0.9151 - val_loss: 0.2512\n",
            "Epoch 11/15\n",
            "\u001b[1m1764/1764\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m491s\u001b[0m 269ms/step - accuracy: 0.9326 - loss: 0.1923 - val_accuracy: 0.9154 - val_loss: 0.2592\n",
            "Epoch 12/15\n",
            "\u001b[1m1764/1764\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m472s\u001b[0m 268ms/step - accuracy: 0.9381 - loss: 0.1795 - val_accuracy: 0.9121 - val_loss: 0.2758\n",
            "Epoch 13/15\n",
            "\u001b[1m1764/1764\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m500s\u001b[0m 267ms/step - accuracy: 0.9435 - loss: 0.1660 - val_accuracy: 0.9113 - val_loss: 0.2743\n",
            "Epoch 14/15\n",
            "\u001b[1m1764/1764\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m468s\u001b[0m 266ms/step - accuracy: 0.9477 - loss: 0.1555 - val_accuracy: 0.9108 - val_loss: 0.3105\n",
            "Epoch 15/15\n",
            "\u001b[1m1764/1764\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m469s\u001b[0m 266ms/step - accuracy: 0.9517 - loss: 0.1449 - val_accuracy: 0.9091 - val_loss: 0.3100\n"
          ]
        }
      ],
      "source": [
        "# Train the optimized LSTM model\n",
        "history = model.fit(\n",
        "    X_train_pad,\n",
        "    y_train,\n",
        "    epochs=15,            # Reduced epochs\n",
        "    batch_size=256,      # Larger batch = faster training\n",
        "    validation_data=(X_test_pad, y_test),\n",
        "    verbose=1\n",
        ")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HE2-KRyMGj7_"
      },
      "source": [
        "Evaluate LSTM Model Performance"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true
        },
        "id": "SVUaDMsWGlqd",
        "outputId": "9fd39b53-d340-476b-ac36-8e0b434729e9"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\u001b[1m3528/3528\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m56s\u001b[0m 16ms/step\n",
            "LSTM Accuracy: 0.9091174855158484\n",
            "\n",
            "LSTM Classification Report:\n",
            "\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "    negative       0.78      0.76      0.77     12800\n",
            "     neutral       0.37      0.23      0.28      5620\n",
            "    positive       0.94      0.97      0.96     94462\n",
            "\n",
            "    accuracy                           0.91    112882\n",
            "   macro avg       0.70      0.65      0.67    112882\n",
            "weighted avg       0.90      0.91      0.90    112882\n",
            "\n"
          ]
        }
      ],
      "source": [
        "# Import evaluation metrics\n",
        "# These are used to measure classification performance\n",
        "from sklearn.metrics import accuracy_score, classification_report\n",
        "\n",
        "# Predict sentiment classes for test data\n",
        "# model.predict returns probabilities, so we take argmax to get class labels\n",
        "y_pred_lstm = model.predict(X_test_pad).argmax(axis=1)\n",
        "\n",
        "# Calculate overall accuracy of the LSTM model\n",
        "# Accuracy shows the percentage of correct predictions\n",
        "lstm_accuracy = accuracy_score(y_test, y_pred_lstm)\n",
        "\n",
        "print(\"LSTM Accuracy:\", lstm_accuracy)\n",
        "\n",
        "# Generate a detailed classification report\n",
        "# This includes precision, recall, F1-score for each sentiment class\n",
        "print(\"\\nLSTM Classification Report:\\n\")\n",
        "print(\n",
        "    classification_report(\n",
        "        y_test,                 # True sentiment labels\n",
        "        y_pred_lstm,            # Predicted sentiment labels\n",
        "        target_names=label_encoder.classes_  # Human-readable class names\n",
        "    )\n",
        ")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WgFwlvIBcpRC"
      },
      "source": [
        "Test LSTM with Custom Text"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true
        },
        "id": "awyXJFCMcnTw",
        "outputId": "59d38807-52b6-4efd-8a73-56937a74b316"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 57ms/step\n",
            "positive\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.12/dist-packages/spacy/pipeline/lemmatizer.py:188: UserWarning: [W108] The rule-based lemmatizer did not find POS annotation for one or more tokens. Check that your pipeline includes components that assign token.pos, typically 'tagger'+'attribute_ruler' or 'morphologizer'.\n",
            "  warnings.warn(Warnings.W108)\n",
            "/usr/local/lib/python3.12/dist-packages/spacy/pipeline/lemmatizer.py:188: UserWarning: [W108] The rule-based lemmatizer did not find POS annotation for one or more tokens. Check that your pipeline includes components that assign token.pos, typically 'tagger'+'attribute_ruler' or 'morphologizer'.\n",
            "  warnings.warn(Warnings.W108)\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 54ms/step\n",
            "negative\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 119ms/step\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.12/dist-packages/spacy/pipeline/lemmatizer.py:188: UserWarning: [W108] The rule-based lemmatizer did not find POS annotation for one or more tokens. Check that your pipeline includes components that assign token.pos, typically 'tagger'+'attribute_ruler' or 'morphologizer'.\n",
            "  warnings.warn(Warnings.W108)\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "neutral\n"
          ]
        }
      ],
      "source": [
        "# Define a function to predict sentiment using the trained LSTM model\n",
        "# This function follows the SAME preprocessing and tokenization pipeline\n",
        "def predict_sentiment_lstm(text):\n",
        "    \"\"\"\n",
        "    Takes raw input text and returns predicted sentiment label using LSTM.\n",
        "    \"\"\"\n",
        "\n",
        "    # Step 1: Clean input text using spaCy preprocessing\n",
        "    cleaned_text = clean_text_spacy_fast(text)\n",
        "\n",
        "    # Step 2: Convert cleaned text into integer sequence using trained tokenizer\n",
        "    sequence = tokenizer.texts_to_sequences([cleaned_text])\n",
        "\n",
        "    # Step 3: Pad the sequence to match LSTM input length\n",
        "    padded_sequence = pad_sequences(\n",
        "        sequence,\n",
        "        maxlen=MAX_SEQUENCE_LENGTH,\n",
        "        padding=\"post\",\n",
        "        truncating=\"post\"\n",
        "    )\n",
        "\n",
        "    # Step 4: Predict sentiment probabilities using LSTM model\n",
        "    prediction_probs = model.predict(padded_sequence)\n",
        "\n",
        "    # Step 5: Get predicted class index (highest probability)\n",
        "    predicted_class = prediction_probs.argmax(axis=1)[0]\n",
        "\n",
        "    # Step 6: Convert numeric label back to original sentiment text\n",
        "    return label_encoder.inverse_transform([predicted_class])[0]\n",
        "\n",
        "\n",
        "# ---- Test the LSTM model with random text inputs ----\n",
        "\n",
        "print(predict_sentiment_lstm(\"This product works perfectly and I am very happy\"))\n",
        "print(predict_sentiment_lstm(\"The appliance stopped working after two days\"))\n",
        "print(predict_sentiment_lstm(\"It is okay, not great but not terrible\"))\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true
        },
        "id": "amKFLTfoX9PS"
      },
      "outputs": [],
      "source": [
        "\n"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}